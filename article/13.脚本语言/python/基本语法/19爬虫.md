一、爬虫
	
	1.这里我将爬虫分为两类：定向爬虫和广域爬虫
	
	2.python3常见的爬虫技术：
		(1)官方提供：urllib
		(2)第三方：requests 和 beautifulSoup4

二、爬虫涉及的其他技术
	
	1.队列
		# 在爬虫程序中, 用到了广度优先搜索(BFS)算法. 这个算法用到的数据结构就是队列.
		# Python的List功能已经足够完成队列的功能, 可以用 append() 来向队尾添加元素, 可以用类似数组的方式来获取队首元素, 
		# 可以用 pop(0) 来弹出队首元素. 但是List用来完成队列功能其实是低效率的, 因为List在队首使用 pop(0) 和 insert()
		# 都是效率比较低的, Python官方建议使用collection.deque来高效的完成队列任务.
		import collections
		
		# 1.创建列表
		li = ["aa", "bb", "cc", "dd"]
		
		# 2.使用列表创建队列
		queue = collections.deque(li)
		print(queue)
		
		# 3.在最后追加元素
		queue.append("ee")
		print(queue)
		
		# 4.队首出列
		ele = queue.popleft()
		print(ele, queue)
		
		# 5.队尾出列
		ele = queue.pop()
		print(ele, queue)
	
	2.集合
		# 在爬虫程序中, 为了不重复爬那些已经爬过的网站, 我们需要把爬过的页面的url放进集合中, 
		# 在每一次要爬某一个url之前, 先看看集合里面是否已经存在. 如果已经存在, 我们就跳过这个url; 
		# 如果不存在, 我们先把url放入集合中, 然后再去爬这个页面.
		import collections
		
		# 1.创建一个集合
		a = {"a", "b", "c", "d"}
		b = {          "c", "d", "e", "f"}
		
		# 2.a集合中不属于b集合的元素
		print(a - b)
		
		# 3.a与b的并集（属于a或b）
		print(a | b)
		
		# 3.a与b的交集（既属于a也属于b）
		print(a & b)
		
		# 4.不同时包含于a和b的元素
		print(a ^ b)
	
	3.gzip
		import urllib.request
		import gzip
		
		def compress(data):# 压缩
		    res = gzip.compress(data) 
		    return res 
		
		def decompress(data):# 解压
		    res = gzip.decompress(data) 
		    return res 
		
		str = "121321sdasdada啊啊啊啊"
		
		# 字符串转byte数组
		b = bytes(str, encoding = "utf-8")
		print(b)
		
		# gzip压缩
		data = compress(b)
		print(data)
		
		# gzip解压
		print(decompress(data))

三、基本操作

	demo1、demo2、demo3、demo4


四、第三方
	
	1.安装
	 	Windows：
		pip install requests
		pip install beautifulsoup4
		
		
		Ubuntu:
		sudo apt-get install python3-pip
		sudo pip3 install requests
		sudo pip3 install beautifulsoup4
	
	2.使用
		import requests
        from bs4 import BeautifulSoup
        
        url = "http://localhost/Account_v2.0/user.do"
        params = {'method':'login', 'name':'admin', 'password':'admin','root':1}
        
        # get请求
        getResponse = requests.get(url, params)
        
        # 使用BeautifulSoup解析
        soup = BeautifulSoup(getResponse.text)
        
        # 获得标题
        print(soup.title.text)
        
        # 获得body
        print(soup.body)
        
        # 获得所有的link标签中属性rel=shortcut icon
        for link in soup.find_all(name = "link", attrs = {'rel':'shortcut icon'}):
            print(link)
            print(link['href'])







