---
sort: 2
---
# 2.Raft

- Raft官网，可以可视化观察选举和日志复制: [https://raft.github.io/](https://raft.github.io/)
- Raft论文：[https://raft.github.io/raft.pdf](https://raft.github.io/raft.pdf)

- [深度解析 Raft 分布式一致性协议](https://juejin.cn/post/6907151199141625870)


## 1.基础

### 1.1.介绍

Raft是Paxos的一个变种，比Paxos更易理解和实现。相比Paxos做出了以下简化
1. Raft将共识算法拆分成3个模块：leader选举、日志复制、安全性，使其更易理解和工程实现。
2. Raft简化了节点的状态，节点只继续状态切换。

### 2.2.应用

目前很多中间件都开始使用Raft协议，例如：etcd、 consul、nacos 和 TiKV。

- etcd。基于Raft实现的分布式kv存储。
- consul 和 nacos，都是将Raft作为可选的一致性协议
- TiKV，作为分布式数据库TiDB的底层实现，采用了Multi Raft，将数据划分为多个region，每个region都是一个Raft集群。

缺点：
1. Raft不适合大数据量存储
2. 节点数量并非越多越好

## 2.选举

Raft采用Quorum机制，将对Raft的操作称之提案，每当发起一个提案，都必须得到超过半数（>N/2）的节点同意才能提交。

### 2.1.节点状态

节点状态：leader（发出提案）、follower（参与决策）、candidate（选举中的临时状态）。

![](../img/共识算法/8afba036.png)

每个节点只有一个状态，只需考虑状态切换，不向paxos存在状态共存的情况。

### 2.2.任期term

任期的就是从开始选举开始计算，直到下一次选举。即，任期是发起两次选举的时间间隔。

任期以整数计算，每经过一次任期（每发起一次选举），任期+1。
任期期间可能无法选举出leader就立即进入下一个任期，重新选举，任期+1。
所以任期期间，可能没有leader，即使有leader也是不可靠的，candidate状态不会接受选举期间leader的写日志。

### 2.3.节点间通讯
Raft定义了节点间通讯采用rpc协议。根据通讯的内容可以分为两类：
1. RequestVote：请求投票，由candidate节点在选举期间发起，向所有的节点发生自己的投票信息。
2. AppendEntries：追加条目，由leader发起，用于日志复制 以及 心跳检查。

通讯的内容会携带当前节点的任期号。根据角色不同，做出反应也不同
1. candidate和leader发现接收到的任期号小于收到的，会判定自己任期号过期了，会立即进入follower状态。
    - 原因：例如发生了脑裂或者其他问题，导致其他节点已经完成了选举，所以先进入follower进行通讯，再进行下一步操作（参与选举或者作为follower工作）
2. follower 接收到的任期号小于自己的，判断收到的信息是过期的，拒绝这条消息。
3. 如果当前节点任期号小了，就会更新自己的任期号。


### 2.4.心跳机制

心跳机制：leader周期性地发送心跳给所有的follower，follower收到后就保持follower状态，如果超时没有收到心跳，
follower节点切换到candidate状态，并发起投票。

follower超时时间：每个节点的心跳超时时间通常是随机的，论文中是介于150ms到300ms之间。

### 2.5.领导选举

选举发生场景
1. 服务启动时，所有节点都是follower，当超时没法收到leader的心跳，就切换到candidate进行选举。
2. 运行期间，leader因为宕机或网络问题中断心跳，follower心跳超时，就切换到candidate进行选举。


选举过程如下：
1. 首先把当前term计数加1，自己的状态切换到candidate。
2. 然后给自己投票并向其它结点发投票请求（term, index）。直到以下三种情况：
    - 它赢得选举，立即切换到leader状态，并向所有节点发送心跳，告诉大家我是leader。
    - 另一个节点成为leader。收到leader的心跳，且任期大于等于自己的任务。
    - 一段时间没有节点成为leader。选举超时，默认为150~300ms。
3. 在选举期间，candidate可能收到来自其它自称为leader的写请求，如果该leader的term不小于candidate的当前term，
   那么candidate承认它是一个合法的leader并切换到follower状态，否则拒绝请求。
4. 如果出现两个candidate得票一样多，则它们都无法获取超过半数投票，这种情况会持续到超时，然后进行新一轮的选举，
   这时失败的概率就很低了，那么首先发出投票请求的的candidate就会得到大多数同意，成为leader。

选举的消息，伪代码
```golang
//请求投票RPCRequest
type RequestVoteRequest struct {
    term int            //自己当前的任期号
    candidateld int     //自己的ID
    lastLogIndex int    //自己最后一个日志号
    lastLogTerm int     //自己最后一个日志的任期
}

//请求投票RPCResponse
type RequestVoteResponse struct {
    term int       //自己当前任期号
    voteGranted bool //自己会不会投票给这 candidate
}    
```

<p style="color: red">如果某个follower超时发起选举，其他follower也会跟着选举吗？</p>

follower状态切换到candidate的条件：follower心跳超时才会转换为candidate状态并发起选举。
如果集群中某个节点因为一些原因，切换状态并发起选举，其他节点不会跟随他一起选举，并返回拒绝投票。
该节点无法正常选举，会一直重试尝试发起选举，无法正常工作。

<p style="color: red">选票逻辑</p>

1. 比较term，term最大的当选
2. term最大有多个相同的时候，lastLogIndex 最大的当选
2. lastLogIndex最大有多个相同的时候，收到的第一条选票中的节点当选

所以在第一次服务启动时，第一个发起选举的节点会当选leader。

## 3.日志复制

### 3.1.复制过程

集群中leader接受所有客户端的请求，所有写入操作都在leader进行。写入操作过程如下：
1. 当leader接收到写入请求后，leader 并发向所有follower发送AppendEntries（term1,index1）请求，要求它们复制日志。
2. 当收到过半的节点的回复（不需要全部节点）时，leader就执行提交命令，然后向给客户端返回成功。
3. 这时 follower 中的日志状态是未提交的
4. leader发送心跳给所有follower（term1,index2），follower检查心跳，发现在同一个term1中。
   - index2>=节点最大index1， 则提交现有所有所有未提交的日志。
   - index2< 节点最大index1， 则提交小于等于index1的日志。

LogIndex 也是全局唯一的自增整数，且一旦服务启动，这个数字只会增加（可能存在回退），不会重置。
LogIndex 与 term 一起组成一个全局唯一标识。

PS: 缺点，论文中的日志复制，leader 和 follower 之间数据是有延迟的。

### 3.2.客户端连接

客户端是如何知道leader节点的？ 常用的方式，启动后向任意节点发生连接请求，会出现三种情况
1. 这个节点就是leader，就建立连接
2. 这个节点是Follower，Follower会保存其他节点的信息，会告诉客户端leader的地址，客户端重新连接。
3. 这个节点不可用，客户端会循环尝试其他的节点，直到找到leader。

### 3.3.数据一致性检查

follower的一致性检查：
- leader发送AppendEntries时，会发送上一条的LogIndex与 term。
- follower如果找不到这个LogIndex与 term，说明follower延迟了，就会拒绝掉这些复制请求
- leader收到拒绝请求后，会向follower发送一个日志列表
- follower 收到列表后，逐一对比，找到自己需要同步的位置，并发起同步请求。

<p style="color: red">follower与leader数据不一致？</p>

每个节点随时都有奔溃、缓慢的情况出现，所以Raft必须保证发生以上情况，日志复制任然可以继续，要保证每一个副本中的日志顺序一致。

原因：
1. follower 同步缓慢。
    - leader发送AppendEntries命令是，follower 由于某些原因，没有及时响应（超时），leader会不断的重试，直到成功。
      这个过程中，leader可能已经提交了日志。
2. follower 崩溃。
    - follower恢复后，会启动一致性检查，与leader对比term 与 commitLogIndex，找到需要同步的开始位置，进行同步。
3. leader 崩溃。
    - 可能是奔溃，或者网络问题导致脑裂，恢复期间，可能已经经过多轮任期，数据已经不是最新的。
    - 可能恢复后，新的leader数据也不是最新的。脑裂的时候，旧leader继续写入数据，导致commitLogIndex大于新leader。

### 3.4.日志压缩

就是定期生成日志快照。快照一般包含以下内容：
- 日志的元数据：最后一条被该快照 apply 的日志 term 及 index
- 状态机：前边全部日志 apply 后最终得到的状态机

日志同步：
1. 当 leader 需要给某个 follower 同步一些旧日志，但这些日志已经被 leader 做了快照并删除掉了时，leader 就需要把该快照发送给 follower。
2. 同样，当集群中有新节点加入，或者某个节点宕机太久落后了太多日志时，leader 也可以直接发送快照，大量节约日志传输和回放时间。

## 4.安全

raft为了便于理解，简化了很多操作，所以在这些操作的过程中，存在一些问题，需要优化。
1. 选举中的限制：每个 candidate 必须在 RequestVote RPC 中携带自己本地日志的最新 (term, index)，如果 follower 发现这个 candidate 的日志还没有自己的新，则拒绝投票给该 candidate。
2. 日志复制的限制：Leader 只允许 commit 包含当前 term 的日志。因为可能选举过程中，上一个leader有数据没有提交，导致follower中的数据也没有提交。
   所以新选举的leader中可能有未提交的数据。选举中选票的index是已提交，而未提交的日志，在后续新leader日志复制过程中，被leader覆盖掉。

## 5.集群成员变更

集群节点的增删操作，一定会有延迟，延迟的过程，就会出现节点之间配置不一致的情况，就可能发生脑裂问题。

### 5.1.联合发布(两阶段切换集群成员配置)

论文中提供的方案，阶段一：
1. 提供变更命令，从leader写入新的配置：C-new
2. leader将新旧配置 进行并集操作，生成一个联合配置 C-old,new 
3. leader以日志的方式发送给所有follower。
4. follower 收到 C-old,new 后立即 apply，当超过半数节点都切换后，leader 将该日志 commit。

阶段二：
1. leader 提交日志后，立即发送C-new给所有follower。
2. follower 接收都C-new后立即应用，如果发现自己被移除了，就会可以退出集群
3. leader 收到当超过半数节点切换成功的响应后，立即返回客户端操作结果。

### 5.2.渐进式变更

一次只变更一个节点。参考具体实现： [https://github.com/ongardie/dissertation](https://github.com/ongardie/dissertation)

## 6.读性能优化

### 6.1.主写从读
raft协助中，leader写操作，日志复制时大部分通过即可，且follower本地日志提交依赖于leader下一次心跳消息。
所以follower中日志是有延迟的，且存在不一致的情况。所以这种主写从读的方式是不成立的。

### 6.1.主写主读

也是存在问题。例如：
1. 日志未提交（还在日志复制过程中），被读取到的话就产生了脏读。
2. 网络分区导致脏读，此时如果连接的是旧leader，用户写入数据后，可能被连接旧leader的其他用户读取到。当集群恢复后这些数据都没了，这就产生了脏读。

从leader读取数据是有必要的，所以以上问题需要优化，方案如下，这些方案任选其一即可。
- Raft log read
- Read Index
- Lease Read
- Follower Read

目标：
1. 保证在读取时的最新 commit index 已经被 follower 提交。
2. 保证在读取时 leader 仍拥有领导权。

### 6.1.1.Raft log read

raft log read。将读操作也作为一种提案，在raft集群中走一遍日志复制过程。这样可以很大程度上保证连接到的leader节点是有效的。

缺点：读操作与写操作一样耗时，性能不好。

### 6.1.2.Read Index

- Leader 接收到读操作，记录下当前的 commit index，称之为 read index。
- 为了确保领导权，主动让 Leader 向 所有 follower 发起一次心跳，同时也是为了让follower上未提交的数据提前提交。
- 当大半follower都回复成功后，即 apply index 大于等于 read index
- 执行读请求，将结果返回给客户端。

就是将心跳与读操作合并一起，很多程度的提升读取性能，这么做为了保证Leader有效，follower数据即使提交。

### 6.1.3.Lease Read

- 设置一个比选举超时更短的时间作为租期，在租期内我们可以相信其它节点一定没有发起选举，集群也就一定不会存在脑裂。
- 在租期内可以直接读主，而租期过期后使用Read Index。
- 下一次读操作使用Read Index后，刷新租期，续租。

缺点：依赖于系统时间，为算法带来一些不确定性，如果时钟发生漂移会引发一系列问题，因此需要谨慎的进行配置。

### 6.1.4.Follower Read

Follower 在收到客户端的读请求时，向 leader 询问当前最新的 commit index，反正所有日志条目最终一定会被同步到自己身上，
follower 只需等待该日志被自己 commit 并 apply 到状态机后，返回给客户端本地状态机的结果即可。


