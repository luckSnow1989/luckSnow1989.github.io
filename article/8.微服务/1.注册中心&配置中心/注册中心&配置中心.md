---
sort: 1
---
# 1.注册中心&配置中心

- [注册中心ZooKeeper、Eureka、Consul 、Nacos对比](https://blog.csdn.net/fly910905/article/details/100023415)
- [主流微服务注册中心浅析和对比](https://developer.aliyun.com/article/698930)

## 注册中心

### 1.常见注册中心

- zookeeper 注册量可以到20K
- Eureka 注册量可以到30K。
- consul
- nacos 注册量可以到200K

|                 | Nacos                      | Eureka      | Consul            | CoreDNS    | Zookeeper  |
| ----------------- | ---------------------------- | ------------- | ------------------- | ------------ | ------------ |
| 一致性协议      | CP+AP                      | AP          | CP                | —         | CP         |
| 健康检查        | TCP/HTTP/MYSQL/Client Beat | Client Beat | TCP/HTTP/gRPC/Cmd | —         | Keep Alive |
| 服务注册&发现   | 支持                       | 支持        | 支持              | 支持       | 支持       |
| 服务治理        | 支持                       | 不支持      | 不支持            | 支持       | 不支持     |
| 负载均衡策略    | 权重 metadata/Selector     | Ribbon      | Fabio             | RoundRobin | —         |
| 雪崩保护        | 有                         | 有          | 无                | 无         | 无         |
| 自动注销实例    | 支持                       | 支持        | 不支持            | 不支持     | 支持       |
| 访问协议        | TCP/HTTP/DNS               | HTTP        | HTTP/DNS          | DNS        | TCP        |
| 监听支持        | 支持                       | 支持        | 支持              | 不支持     | 支持       |
| 多数据中心      | 支持                       | 支持        | 支持              | 不支持     | 不支持     |
| 跨注册中心同步  | 支持                       | 不支持      | 支持              | 不支持     | 不支持     |
| SpringCloud集成 | 支持                       | 支持        | 支持              | 不支持     | 不支持     |
| Dubbo集成       | 支持                       | 不支持      | 不支持            | 不支持     | 支持       |
| K8S集成         | 支持                       | 不支持      | 支持              | 支持       | 不支持     |
| 多语言能力      | 支持                       | 支持        | 支持              | 不支持     | 支持的不好 |
| watch 支持      | 支持                       | 不支持      | 支持              | 支持       | 支持       |

时效性：

- zk，时效性更好，注册或者是挂了，一般秒级就能感知到
- eureka，默认配置非常糟糕，服务发现感知要到几十秒，甚至分钟级别
  上线一个新的服务实例，到其他人可以发现他，极端情况下，可能要1分钟的时间，ribbon去获取每个服务上缓存的eureka的注册表进行负载均衡
  服务故障，隔60秒才去检查心跳，发现这个服务上一次心跳是在60秒之前，隔60秒去检查心跳，超过90秒没有心跳，才会认为他死了。
  30秒会更新缓存，30秒其他服务才会来拉取最新的注册表。三分钟都过去了，如果你的服务实例挂掉了，此时别人感知到，可能要两三分钟的时间，一两分钟的时间，很漫长
- nacos。2.X以前的架构设计参考了eureka，服务发现速度稍慢。2.X之后使用了rpc协议，服务感知是实时的。

容量：
- zk，不适合大规模的服务实例，因为服务上下线的时候，需要瞬间推送数据通知到所有的其他服务实例
  所以一旦服务规模太大，到了几千个服务实例的时候，会导致网络带宽被大量占用
- eureka，也很难支撑大规模的服务实例，因为每个eureka实例都要接受所有的请求，实例多了压力太大，扛不住，也很难到几千服务实例
  之前dubbo技术体系都是用zk当注册中心，spring cloud技术体系都是用eureka当注册中心这两种是运用最广泛的

watch支持:就是客户单监听服务端的变化情况。
- zk 通过订阅监听来实现（服务端主动推送）
- eureka，不支持，客户端主动发送心跳检测并拉取实例信息，有时间延迟。
- nacos， 2.X以前通过http长轮询的方式。2.X之后通过rpc实时通讯。

适用场景:
- zk：    通用性强，基于CP，不适合大规模集群使用，尽可能不要使用，dubbo服务推荐使用nacos。
- eureka：不具备隔离性，适合小规模部署使用，特别是单个微服务内部使用，禁止作为公共服务使用。
- nacos:  性能卓越、功能性强、具备用户和隔离性，运行时同时支持CP和AP，可以用于大规模生产环境。

### 2.我们对于注册中心的诉求

1. 注册中心的高可用诉求

问：CAP中注册中心一定要保证的是谁？
是分区容错性（P）。分布式服务依赖网络进行节点连通，在遇到任何网络分区故障时，仍然需要能够保证系统可以对外提供服务（一致性 或可用性的服务），
除非是整个网络环境都发生了故障。我们不允许当节点间通信出现故障时，被孤立节点都不能提供服务。最简单的，可以让所有节点拥有所有数据。
但是我们对于注册中心的可用性诉求，要比数据一致性要大的多。也可以说，生产环境，我们是无法容忍注册中心无法保证可用性。这对实际生产的影响是灾难性的。

2. 注册中心的容灾诉求

在实践中，注册中心不能因为自身的任何原因破坏服务之间本身的可连通性。所以，如果整个注册中心宕机了呢？
因为，zk只能存在一个leader，Leader是所有写操作的入口，所有zk是无法实现跨机房、跨地域容灾的。

3、服务规模、容量的增长

互联网的发展，有一定的偶发性，现在的节点上限、带宽能满足业务发展，1年后也能满足么? 3年后呢？

当扛不住后，ZK能水平扩展么？

#### 3.1.3.案例

- 2015年JD 大促时由Zookeeper引发服务的全链路雪崩

zookeeper实现的是CP原则。当在Leader选举过程中或一些极端情况下，整个服务是不可用的。

原因：大促期间由于秒杀活动多，增加了很多服务节点。这个时候zk的某个节点挂了或者zk相关的网络被打满了，导致zk开始选举了。
出现部分服务不可用的问题，负责人开始重启服务，想要连接上zk，导致服务不断的重试连接zk。选举变得更慢。
用户请求开始向正常的服务上打，导致正常的服务扛不住陆续崩掉，直到所有的服务都不可用。

#### 3.1.4.自研注册中心注意哪些

既然要自研注册中心了 那么肯定要开发其对应的客户端以及服务端， 两端都要考虑各自的东西

1.客户端

- 服务拉取：每次肯定不是全量拉取，这个时候你就要思考服务端有可能提供一个最近的变更队列，供客户端拉取。并且一定要有一个用于校验拉取增量数据之后数据是否完整的指标数据，用于校验数据是否有异常，异常则做一次全量拉取。
- 心跳发送：告诉服务端自己是否存活进行服务的一个续约。
- 服务下线：进行服务下线，修改运行状态的标记位，当然这个标记位要保证其可见性。

2.服务端

- 服务注册：要对java并发包的拥有着深刻的理解，在服务注册表一定一定要注意读写并发控制，既要保证线程安全，同时也要降低锁的争用，最大限度保证其性能。
- 对所注册服务的健康检查：在单位时间内，如果所注册服务没进行服务的续约，则要将该服务下线。
- 集群同步：要根据自己实际的业务需求，制定合适的集群架构方案，在这个前提下，在制定合适的数据传输方案，保证吞吐量。

还有就是要考虑，注册中心与主流技术框架兼容性。


## 【配置中心】Apollo

Apollo（阿波罗）是一款可靠的分布式配置管理中心，诞生于携程框架研发部，能够集中化管理应用不同环境、不同集群的配置，
配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。

源码： [https://github.com/ctripcorp/apollo](https://github.com/ctripcorp/apollo)

中文文档：[https://www.apolloconfig.com/#/zh/README](https://www.apolloconfig.com/#/zh/README)

https://www.cnblogs.com/andyfengzp/p/7243847.html

## 【配置中心】confd

源码： [ https://github.com/sumory/confd]( https://github.com/sumory/confd)

Confd是一个轻量级的工具，用于管理分布式系统中的配置文件。它通过将配置文件和模板分离来解决配置管理的挑战。
Confd监视由Etcd、Zookeeper、Consul等后端存储的配置更改，然后根据定义的模板生成配置文件，并将其分发到系统中的所有节点。
Confd还支持从命令行或环境变量中读取配置参数，并将其注入到模板中。

在实践中，Confd可以用于管理诸如Nginx、Apache等Web服务器的配置文件，以及运行在Docker或Kubernetes容器中的应用程序的配置文件。
Confd还可以通过与Vault等密钥管理工具的集成来提供安全的配置存储和传输。

## 【注册中心】consul

### 1.介绍
consul-常用命令		https://www.cnblogs.com/xiajq/p/11687270.html

Consul详细命令  	https://blog.csdn.net/y435242634/article/details/78769147

### 2.教程
```text
第一章 consul简介：				https://www.cnblogs.com/java-zhao/p/5356507.html
第二章 consul的安装和启动：		https://www.cnblogs.com/java-zhao/p/5356744.html
第三章 consul服务注册与服务查询：	https://www.cnblogs.com/java-zhao/p/5359806.html
第四章 consul cluster：			https://www.cnblogs.com/java-zhao/p/5375132.html
第五章 consul key/value：		https://www.cnblogs.com/java-zhao/p/5381892.html
第六章 consul UI：				https://www.cnblogs.com/java-zhao/p/5387105.html
附1 consul常用命令+常用选项：	https://www.cnblogs.com/java-zhao/p/5378876.html

启动命令脚本：
	@echo off
	rem 简单启动：	consul agent -dev
	rem 其中X.X.X.X为服务器ip,即可使用http://X.X.X.X:8500 访问ui
	consul.exe agent -server -ui -bootstrap -client 0.0.0.0 -data-dir="D:\java\consul\data" -bind 127.0.0.1
```

consul 是B/C架构。自带 客户端 与 服务端。

### 3.原理

consul 使用的是 Gossip 协议。

Gossip 协议被广泛应用在多个知名项目中，比如 Redis Cluster 集群版，Apache Cassandra，AWS Dynamo。

Consul 天然支持多数据中心，但是多数据中心内的服务数据并不会跨数据中心同步，各个数据中心的 Server 集群是独立的，
Consul 提供了 Prepared Query 功能，它支持根据一定的策略返回多数据中心下的最佳的服务实例地址，使你的服务具备跨数据中心容灾。

Consul 支持以下三种模式的读请求：
- 默认（default）。默认是此模式，绝大部分场景下它能保证数据的强一致性。
  但在老的 Leader 出现网络分区被隔离、新的 Leader 被选举出来的一个极小时间窗口内，可能会导致 stale read。
  这是因为 Consul 为了提高读性能，使用的是基于 Lease 机制来维持 Leader 身份，避免了与其他节点进行交互确认的开销。

- 强一致性（consistent）。强一致性读与 etcd 默认线性读模式一样，每次请求需要集群多数节点确认 Leader 身份，
  因此相比 default 模式读，性能会有所下降。

- 弱一致性（stale)。任何节点都可以读，无论它是否 Leader。可能读取到陈旧的数据，类似 etcd 的串行读。
  这种读模式不要求集群有 Leader，因此当集群不可用时，只要有节点存活，它依然可以响应读请求。

## 【注册中心】etcd

官网: [https://etcd.io/](https://etcd.io/)

### 1.介绍

etcd 是一个一致的分布式键值存储。在分布式系统中主要用作单独的协调服务。并且旨在保存可以完全放入内存的少量数据（默认2G）。

随着CoreOS和Kubernetes等项目在开源社区日益火热，它们项目中都用到的etcd组件作为一个高可用、强一致性的服务发现存储仓库，渐渐为开发人员所关注。

etcd作为一个受到Zookeeper启发的项目，有以下特点。
- 简单：基于HTTP+JSON的API让你用curl命令就可以轻松使用。
- 安全：可选SSL客户认证机制。
- 快速：每个实例每秒支持一千次写操作。
- 可信：使用Raft算法充分实现了分布式。

线性一致性。通过将所有写请求转发到leader上实现。

etcd 使用go开发，属于非常新的框架，没有交给Apache。目前还处于迭代阶段，借由云原生应运而生，后期可期。

### 2.使用教程

入门教程：https://juejin.cn/post/6844904031186321416

基本命令与zk基本一致。

- [etcd学习(1)-etcd的使用场景](https://www.cnblogs.com/ricklz/p/15033193.html)
- [etcd学习(2)-etcd中的watch源码阅读](https://www.cnblogs.com/ricklz/p/15037925.html)
- [etcd学习(3)-grpc使用etcd做服务发现](https://www.cnblogs.com/ricklz/p/15059497.html)
- [etcd学习(4)-centos7中部署etcd](https://www.cnblogs.com/ricklz/p/15074924.html)
- [etcd学习(5)-etcd的Raft一致性算法原理](https://www.cnblogs.com/ricklz/p/15094389.html)
- [etcd学习(6)-etcd实现raft源码解读](https://www.cnblogs.com/ricklz/p/15155095.html)
- [etcd学习(7)-etcd中如何实现线性一致性](https://www.cnblogs.com/ricklz/p/15204381.html)
- [etcd学习(8)-etcd中的lease](https://www.cnblogs.com/ricklz/p/15232204.html)
- [etcd学习(9)-etcd中的存储实现](https://www.cnblogs.com/ricklz/p/15253404.html)
- [etcd学习(10)-etcd对比Consul和zooKeeper如何选型](https://www.cnblogs.com/ricklz/p/15292306.html)


### 3.特性
- Lease 机制

即租约机制（TTL，Time To Live），Etcd 可以为存储的 Key-Value 对设置租约，当租约到期，Key-Value 将失效删除；
同时也支持续约，通过客户端可以在租约到期之前续约，以避免 Key-Value 对过期失效。Lease 机制可以保证分布式锁的安全性，
为锁对应的 Key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放。

- Revision 机制

每个 Key 带有一个 Revision 号，每进行一次事务便加一，因此它是全局唯一的，如初始值为 0，进行一次 put(key, value)，
Key 的 Revision 变为 1，同样的操作，再进行一次，Revision 变为 2；换成 key1 进行put(key1, value)操作，Revision将变为 3；
这种机制有一个作用：通过 Revision 的大小就可以知道写操作的顺序。在实现分布式锁时，多个客户端同时抢锁，
根据 Revision 号大小依次获得锁，可以避免 “羊群效应” （也称“惊群效应”），实现公平锁。

- Prefix 机制

即前缀机制，也称目录机制，例如，一个名为 /mylock 的锁，两个争抢它的客户端进行写操作，
实际写入的Key分别为：key1="/mylock/UUID1",key2="/mylock/UUID2"，其中，UUID表示全局唯一的ID，确保两个Key的唯一性。
很显然，写操作都会成功，但返回的Revision不一样，那么，如何判断谁获得了锁呢？通过前缀“/mylock”查询，
返回包含两个Key-Value对的Key-Value列表，同时也包含它们的Revision，通过Revision大小，客户端可以判断自己是否获得锁，
如果抢锁失败，则等待锁释放（对应的 Key 被删除或者租约过期），然后再判断自己是否可以获得锁。

- Watch 机制

即监听机制，Watch机制支持监听某个固定的Key，也支持监听一个范围（前缀机制），当被监听的Key或范围发生变化，客户端将收到通知；
在实现分布式锁时，如果抢锁失败，可通过Prefix机制返回的Key-Value列表获得Revision比自己小且相差最小的 Key（称为 Pre-Key），
对Pre-Key进行监听，因为只有它释放锁，自己才能获得锁，如果监听到Pre-Key的DELETE事件，则说明Pre-Key已经释放，自己已经持有锁。

### 4.总结

etcd 默认只保存 1000 个历史事件，所以不适合有大量更新操作的场景，这样会导致数据的丢失。

相比于 zookeeper，etcd 使用起来要简单很多。不过要实现真正的服务发现功能，etcd 还需要和其他工具（比如 registrator、confd 等）一起使用来实现服务的自动注册和更新。

相比于zk，etcd 采用 raft 协议作为一致性算法，比基于Paxos的zab协议实现更加简单，选主阶段时间更短。

[ETCD的内存问题与最佳实践](https://coolshell.cn/articles/22242.html)

## 如何设计配置中心

[springboot配置中心](https://code2life.top/2021/05/31/0061-spring-boot-dynamic-config/)

配置中心是微服务系统必不可少的组件之一，乍一看好像没多少技术含量，很多人角色配置中心就是一个kv存储，可是，真的是这样吗？
以Java Spring技术栈为例，主流的配置中心有阿里的Nacos、携程的Apollo、以及Spring Cloud Config Server。我们拆解一下其中共通的技术点：

服务端：
- 认证和权限控制：某个服务可以拿到哪些Key？人员的增删改查权限如何控制？
- 存储层的选型：文件系统，Git仓库，数据库？
- 安全性：传输加TLS，密钥需要落盘加密，本身用来加密密钥的密钥如何安全存储？
- 高可用、数据一致性：多实例部署，甚至跨区域同步，进而又带来分布式存储一致性问题，如何解决？
- 版本控制：修改记录需要保留，随时可能回退到历史版本，另外，灰度版本的配置隔离如何实现？
- 即时发布：配置变更后，实时通知客户端进行变更。

客户端：
- SDK如何兼容不同的技术栈？
- 大量客户端同时启动，如何做并发控制？同时还要尽可能减少额外的请求，对服务启动时间的负面影响？
- 如何实现与本地配置的优先级控制、合并、缓存、变化实时感知？
- 热更新

